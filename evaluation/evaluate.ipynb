{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLIPScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_path = \"../data/results/default-experiment/results_clipscore.tsv\"\n",
    "result_df = pd.read_csv(os.path.join(os.getcwd(), result_path), sep=\"\\t\", index_col=[0,1])\n",
    "\n",
    "prompt_num = result_df.index.get_level_values(0).unique().shape[0]\n",
    "optimization_step_num = result_df.index.get_level_values(1).unique().shape[0] - 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average CLIP Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>image_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33.474657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30.980562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30.850779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30.695939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30.562657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>30.710908</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              score\n",
       "image_id           \n",
       "0         33.474657\n",
       "1         30.980562\n",
       "2         30.850779\n",
       "3         30.695939\n",
       "4         30.562657\n",
       "5         30.710908"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_clipscore_results = result_df.groupby(level=1).mean()\n",
    "avg_clipscore_results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimized Image vs. Original Image Score\n",
    "Count the number of occurances per optimization steps where the similarity score of the optimized generated image and the user prompt is higher then the image generated based on the original user prompt.\n",
    "This metric measures if optimizing the prompt using our model yields better results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization step 1: 30.9% of prompts improved\n",
      "Optimization step 2: 29.5% of prompts improved\n",
      "Optimization step 3: 30.1% of prompts improved\n",
      "Optimization step 4: 30.6% of prompts improved\n",
      "Optimization step 5: 30.7% of prompts improved\n"
     ]
    }
   ],
   "source": [
    "original_scores = result_df.loc(axis=0)[:,0][\"score\"].values\n",
    "for optimization_step in range(1, optimization_step_num+1):\n",
    "    optimization_scores = result_df.loc(axis=0)[:,optimization_step][\"score\"].values\n",
    "    improvement_count = np.sum(optimization_scores > original_scores)\n",
    "    improvement_percentage = improvement_count / prompt_num\n",
    "    print(f\"Optimization step {optimization_step}: {improvement_percentage*100:.1f}% of prompts improved\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Current Optimized Image vs. Previous Generations Score\n",
    "Count the number of occurances where the generated image of the current optimization steps achieves the highest similarity with the original user prompt compared to all previous generations. \n",
    "This metric measures if running the optimization loop several times improves results and if/when a convergence is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization step 1: 30.9% of prompts improved compared to previous generation steps\n",
      "Optimization step 2: 20.6% of prompts improved compared to previous generation steps\n",
      "Optimization step 3: 17.6% of prompts improved compared to previous generation steps\n",
      "Optimization step 4: 14.6% of prompts improved compared to previous generation steps\n",
      "Optimization step 5: 11.9% of prompts improved compared to previous generation steps\n"
     ]
    }
   ],
   "source": [
    "for optimization_step in range(1, optimization_step_num+1):\n",
    "    previous_scores = result_df.loc(axis=0)[:,:optimization_step-1][\"score\"].max(level=0).values\n",
    "    optimization_scores = result_df.loc(axis=0)[:,optimization_step][\"score\"].values\n",
    "    improvement_count = np.sum(optimization_scores > previous_scores)\n",
    "    improvement_percentage = improvement_count / prompt_num\n",
    "    print(f\"Optimization step {optimization_step}: {improvement_percentage*100:.1f}% of prompts improved compared to previous generation steps\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Current Optimized Image vs. Previous Generations Score\n",
    "Count the number of occurances where the generated image of a given optimization steps achieves the highest similarity with the original user prompt compared to all other generated images.\n",
    "This metric measures which optimization steps tends to yield the most fitting images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization step 1: 13.9% of prompts achieved best overall performance\n",
      "Optimization step 2: 12.1% of prompts achieved best overall performance\n",
      "Optimization step 3: 11.9% of prompts achieved best overall performance\n",
      "Optimization step 4: 12.5% of prompts achieved best overall performance\n",
      "Optimization step 5: 11.9% of prompts achieved best overall performance\n"
     ]
    }
   ],
   "source": [
    "for optimization_step in range(1, optimization_step_num+1):\n",
    "    previous_scores = result_df.loc(axis=0)[:,:optimization_step-1][\"score\"].max(level=0).values\n",
    "    if optimization_step == optimization_step_num:\n",
    "        next_scores = np.zeros(prompt_num)\n",
    "    else:\n",
    "        next_scores = result_df.loc(axis=0)[:,optimization_step+1:][\"score\"].max(level=0).values\n",
    "    combined_max = np.max([previous_scores, next_scores], axis=0)\n",
    "    optimization_scores = result_df.loc(axis=0)[:,optimization_step][\"score\"].values\n",
    "    improvement_count = np.sum(optimization_scores > combined_max)\n",
    "    improvement_percentage = improvement_count / prompt_num\n",
    "    print(f\"Optimization step {optimization_step}: {improvement_percentage*100:.1f}% of prompts achieved best overall performance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
