import os, sys
import enum
import openai
from transformers import pipeline
from transformers import AutoTokenizer, AutoModelForCausalLM
import time

class LanguageModelType(enum.Enum):
    chat_gpt = "chat_gpt"
    davinci_003 = "davinci-003"
    Cerebras1B = "Cerebras1B"

def load_language_model(**kwargs):
    """
    Load specified LLM model
    
    Parameters:
        model (str): name of model to load
        kwargs (dict): additional config arguments to pass to model
    Returns:
        LanguageModel: instanciated and configured language model sub-class
    """
    print("Loading language model")

    # TODO instanciate language model sub-class from given model name

    model_name = kwargs.get('model', LanguageModelType.chat_gpt.value)
    if model_name == LanguageModelType.chat_gpt.value:
        language_model = ChatGPT(**kwargs)
    elif model_name == LanguageModelType.davinci_003.value:
        language_model = Davinci003(**kwargs)
    elif model_name == LanguageModelType.Cerebras1B.value:
        language_model = Cerebras1B(**kwargs)
    else:
        raise ValueError(f"Unknown language model {model_name}")

    return language_model


class LanguageModel():
    """
    Base class for language models
    """

    def __init__(self, **kwargs):

        self.template = self.load_template(kwargs.get("template", "config/templates/default_template.txt"))
        self.similarity_template = self.load_template(kwargs.get("similarity_template", "config/templates/default_similarity_template.txt"))
    
    def check_similarity(self, user_prompt: str, image_caption: str):
        """
        Check similarity between user prompt and image caption
        Determines whether caption is suffiently similar to user prompt to terminate generation process

        Parameters:
            user_prompt (str): user prompt
            image_caption (str): image caption
        Returns:
            bool: True if caption is sufficiently similar to user prompt, False otherwise
        """

        similarity_prompt = self.get_similarity_prompt(user_prompt, image_caption)
        similarity_response = self.query_language_model(similarity_prompt)

        return 1 if "Yes" in similarity_response else 0


    def generate_optimized_prompt(self, user_prompt: str, image_caption: str, previous_prompts: list = []):
        """
        Generate optimized prompt given original user prompt, image caption, and possibly previous prompts

        Parameters:
            user_prompt (str): user prompt
            image_caption (str): image caption
            previous_prompts (list): list of prompts previously generated by model
        Returns:
            str: optimized prompt
        """

        LLM_prompt = self.get_language_prompt(user_prompt, image_caption, previous_prompts)
        LLM_response = self.query_language_model(LLM_prompt)

        return LLM_response
    
    def load_template(self, template_file: str):
        """
        Load templates from file

        Parameters:
            template_file (str): path to template file
        Returns:
            str: template
        """

        with open(template_file, "r") as f:
            template = f.read()
        
        return template

    def get_language_prompt(self, user_prompt: str, image_caption: str, previous_prompts: list = []):
        """
        Generate language prompt given original user prompt, image caption, and possibly previous prompts

        Parameters:
            user_prompt (str): user prompt
            image_caption (str): image caption
            previous_prompts (list): list of prompts previously generated by model
        Returns:
            str: prompt for language model
        """

        # Replace <USER_PROMPT> in template with user prompt
        prompt = self.template.replace("<USER_PROMPT>", user_prompt)
        # Replace <IMAGE_CAPTION> in template with image caption
        prompt = prompt.replace("<IMAGE_CAPTION>", image_caption)
        # Add optional text for previous prompts
        if len(previous_prompts) > 0:
            #TODO implement properly depending on ultimate syntax for previous prompt definition
            previous_prompt = ""
            for prompt in previous_prompts:
                prompt_prefix = "" #TODO: add prefix for previous prompts
                prompt_suffix = "" #TODO: add suffix for previous prompts
                previous_prompt += prompt_prefix + prompt + prompt_suffix
            prompt = prompt.replace("<PREVIOUS_PROMPTS>", previous_prompt)

        return prompt
    
    def get_similarity_prompt(self, user_prompt: str, image_caption: str):
        """
        Generate similarity prompt given original user prompt and image caption

        Parameters:
            user_prompt (str): user prompt
            image_caption (str): image caption
        Returns:
            str: prompt for similarity check
        """

        # Replace <USER_PROMPT> in template with user prompt
        prompt = self.similarity_template.replace("<USER_PROMPT>", user_prompt)
        # Replace <IMAGE_CAPTION> in template with image caption
        prompt = prompt.replace("<IMAGE_CAPTION>", image_caption)

        return prompt
    
    def query_language_model(self, prompt: str):
        """
        Query language model with prompt
        -> to be implemented by sub-class

        Parameters:
            prompt (str): prompt to query language model with
        Returns:
            str: generated text
        """

        #TODO query language model with prompt -> to be overwritten by sub-class

        raise NotImplementedError
    
    def reset(self):
        """
        Optional method to reset model between generations
        """
        pass

    def load_api_key(self, path: str):
        """
        Load API key from file

        Parameters:
            path (str): path to API key file
        """

        if not os.path.exists(path):
            raise ValueError(f"API key file {path} does not exist")

        with open(path, "r") as f:
            self.api_key = f.read()


class ChatGPT(LanguageModel):
    """
    Wrapper for OpenAI ChatGPT API
    """

    def __init__(self, **kwargs):

        super().__init__(**kwargs)
        self.model = LanguageModelType.chat_gpt.value
        self.load_api_key(kwargs.get("api_key_path", "config/openai_api_key.txt"))
        self.token_usage = 0
        self.role = self.load_template(kwargs.get("system_prompt", "config/templates/model_role.txt"))
        self.similarity_role = self.load_template(kwargs.get("system_sim_prompt", "config/templates/model_role_similarity.txt"))
        openai.api_key = self.api_key

    def get_language_prompt(self, user_prompt: str, image_caption: str, previous_prompts: list = []):
        """
        Generate language prompt given original user prompt, image caption, and possibly previous prompts and role of the model

        Parameters:
            user_prompt (str): user prompt
            image_caption (str): image caption
            previous_prompts (list): list of prompts previously generated by model
            role (str): role of model
        Returns:
            str: prompt for language model
        """

        message = [{"role": "system", "content": self.role}]

        # Replace <USER_PROMPT> in template with user prompt
        prompt = self.template.replace("<USER_PROMPT>", user_prompt)
        # Replace <IMAGE_CAPTION> in template with image caption
        prompt = prompt.replace("<IMAGE_CAPTION>", image_caption)

        message.append({"role": "user", "content": prompt})

        return message

    def get_similarity_prompt(self, user_prompt: str, image_caption: str):
        """
        Generate similarity prompt given original user prompt and image caption

        Parameters:
            user_prompt (str): user prompt
            image_caption (str): image caption
        Returns:
            str: prompt for similarity check
        """
        message = [{"role": "system", "content": self.similarity_role}]

        # Replace <USER_PROMPT> in template with user prompt
        prompt = self.similarity_template.replace("<USER_PROMPT>", user_prompt)
        # Replace <IMAGE_CAPTION> in template with image caption
        prompt = prompt.replace("<IMAGE_CAPTION>", image_caption)

        message.append({"role": "user", "content": prompt})

        return message

    def query_language_model(self, prompt: str):
        """
        Query language model with prompt

        Parameters:
            prompt (str): prompt to query language model with
        Returns:
            str: generated text
        """
        error_counter = 0
        while True:
            try:
                generated_prompt = openai.ChatCompletion.create(
                    model="gpt-3.5-turbo",
                    messages=prompt)
                break
            except Exception as e:
                print(f"Encountered OpenAI exception {e}")
                if error_counter > 5:
                    raise e
                else:
                    time.sleep(60)
                error_counter += 1

        self.token_usage += generated_prompt['usage']['total_tokens']

        return generated_prompt['choices'][0]['message']['content']

    def reset(self):
        """
        Optional method to reset model between generations
        """
        pass

class Davinci003(LanguageModel):
    """
    Wrapper for OpenAI ChatGPT API
    """

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.model = LanguageModelType.davinci_003.value
        self.load_api_key(kwargs.get("api_key_path", "config/openai_api_key.txt"))
        self.token_usage = 0
        openai.api_key = self.api_key

    def get_language_prompt(self, user_prompt: str, image_caption: str, previous_prompts: list = []):
        """
        Generate language prompt given original user prompt, image caption, and possibly previous prompts and role of the model

        Parameters:
            user_prompt (str): user prompt
            image_caption (str): image caption
            previous_prompts (list): list of prompts previously generated by model
            role (str): role of model
        Returns:
            str: prompt for language model
        """

        # Replace <USER_PROMPT> in template with user prompt
        prompt = self.template.replace("<USER_PROMPT>", user_prompt)
        # Replace <IMAGE_CAPTION> in template with image caption
        prompt = prompt.replace("<IMAGE_CAPTION>", image_caption)

        return prompt

    def get_similarity_prompt(self, user_prompt: str, image_caption: str):
        """
        Generate similarity prompt given original user prompt and image caption

        Parameters:
            user_prompt (str): user prompt
            image_caption (str): image caption
        Returns:
            str: prompt for similarity check
        """

        # Replace <USER_PROMPT> in template with user prompt
        prompt = self.similarity_template.replace("<USER_PROMPT>", user_prompt)
        # Replace <IMAGE_CAPTION> in template with image caption
        prompt = prompt.replace("<IMAGE_CAPTION>", image_caption)

        return prompt

    def query_language_model(self, prompt: str):
        """
        Query language model with prompt

        Parameters:
            prompt (str): prompt to query language model with
        Returns:
            str: generated text
        """
        generated_prompt = openai.Completion.create(engine='text-davinci-003', prompt=prompt, stop='.')

        self.token_usage += generated_prompt['usage']['total_tokens']

        return generated_prompt['choices'][0]['text'].strip()

    def reset(self):
        """
        Optional method to reset model between generations
        """
        pass


class Cerebras1B(LanguageModel):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)

        self.tokenizer = AutoTokenizer.from_pretrained("cerebras/Cerebras-GPT-1.3B")
        self.model = AutoModelForCausalLM.from_pretrained("cerebras/Cerebras-GPT-1.3B").to("cuda")
        self.pipeline = pipeline("text-generation", model=self.model, tokenizer=self.tokenizer, device=0)

    def query_language_model(self, prompt: str):
        """
        Query language model with prompt

        Parameters:
            prompt (str): prompt to query language model with
        Returns:
            str: generated text
        """

        generated_text = self.pipeline(prompt, max_length=500, do_sample=False, no_repeat_ngram_size=2)[0]

        return generated_text['generated_text']